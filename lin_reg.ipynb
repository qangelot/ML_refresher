{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function\n",
    "We can measure the accuracy of our hypothesis function by using a cost function. This takes an average difference (actually a fancier version of an average) of all the results of the hypothesis with inputs from x's and the actual output y's.\n",
    "\n",
    "\\begin{align*}&J(\\theta_0, \\theta_1) = \\dfrac {1}{2m} \\displaystyle \\sum _{i=1}^m \\left ( \\hat{y}_{i}- y_{i} \\right)^2 = \\dfrac {1}{2m} \\displaystyle \\sum _{i=1}^m \\left (h_\\theta (x_{i}) - y_{i} \\right)^2 \\newline\\end{align*}\n",
    "\n",
    "To break it apart, it is $\\frac{1}{2}\\bar{x}$ where $\\bar{x}$ is the mean of the squares of $h_\\theta (x_{i}) - y_{i}$, or the difference between the predicted value and the actual value.\n",
    "\n",
    "This function is otherwise called the \"Squared error function\", or \"Mean squared error\". The mean is halved $\\left(\\frac{1}{2}\\right)$ as a convenience for the computation of the gradient descent, as the derivative term of the square function will cancel out the $\\frac{1}{2}$ term. \n",
    "\n",
    "The following image summarizes what the cost function does:\n",
    "<br>\n",
    "<img src='files/lin_reg.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "The way we minimize our cost function is by taking the derivative (the tangential line to a function) of it. The slope of the tangent is the derivative at that point and it will give us a direction to move towards. We make steps down the cost function in the direction with the steepest descent. The size of each step is determined by the parameter α, which is called the learning rate.\n",
    "\n",
    "For example, the distance between each 'star' in the graph above represents a step determined by our parameter α. A smaller α would result in a smaller step and a larger α results in a larger step. The direction in which the step is taken is determined by the partial derivative of $J(\\theta_0,\\theta_1)$. \n",
    "\n",
    "Depending on where one starts on the graph, one could end up at different points. The image above shows us two different starting points that end up in two different places.\n",
    "\n",
    "The gradient descent algorithm is:\n",
    "\n",
    "repeat until convergence:\n",
    "\n",
    "\\begin{align*}& \\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta_0, \\theta_1) \\newline\\end{align*}\n",
    "where\n",
    "\n",
    "j=0,1 represents the feature index number.\n",
    "\n",
    "At each iteration j, one should simultaneously update the parameters \\begin{align*}& \\theta_1, \\theta_2,...,\\theta_nθ\n",
    "1\n",
    "​\t ,θ \n",
    "2\n",
    "​\t ,...,θ \n",
    "n\n",
    "​ \t .   \n",
    "\\newline\\end{align*} Updating a specific parameter prior to calculating another one on the \\begin{align*}& j^{(th)}j \n",
    "(th)  \\newline\\end{align*}\n",
    "  iteration would yield to a wrong implementation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Intuition\n",
    "\n",
    "In this video we explored the scenario where we used one parameter $\\theta_1$\n",
    "​\t  and plotted its cost function to implement a gradient descent. Our formula for a single parameter was :\n",
    "\n",
    "Repeat until convergence:\n",
    "\n",
    "\\begin{align*}& \\theta_1:=\\theta_1-\\alpha \\frac{d}{d\\theta_1} J(\\theta_1)\n",
    "\\newline\\end{align*}\n",
    "Regardless of the slope's sign for \n",
    "\\begin{align*}& \\frac{d}{d\\theta_1} J(\\theta_1), \\newline\\end{align*}\n",
    "θ1\n",
    "​\t eventually converges to its minimum value. The following graph shows that when the slope is negative, the value of θ1\n",
    "​\t  increases and when it is positive, the value of θ1\n",
    "​\t  decreases.\n",
    "\n",
    "On a side note, we should adjust our parameter \\alphaα to ensure that the gradient descent algorithm converges in a reasonable time. Failure to converge or too much time to obtain the minimum value imply that our step size is wrong.\n",
    "How does gradient descent converge with a fixed step size $\\alpha$?\n",
    "The intuition behind the convergence is that \\begin{align*}& \\frac{d}{d\\theta_1} J(\\theta_1)  \\newline\\end{align*} approaches 0 as we approach the bottom of our convex function. At the minimum, the derivative will always be 0 and thus we get:\n",
    "\n",
    "\\begin{align*}&  \\theta_1:=\\theta_1-\\alpha * 0  \\newline\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent For Linear Regression\n",
    "\n",
    "When specifically applied to the case of linear regression, a new form of the gradient descent equation can be derived. We can substitute our actual cost function and our actual hypothesis function and modify the equation to :\n",
    "\n",
    "\\begin{align*} \\text{repeat until convergence: } \\lbrace & \\newline \\theta_0 := & \\theta_0 - \\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{m}(h_\\theta(x_{i}) - y_{i}) \\newline \\theta_1 := & \\theta_1 - \\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{m}\\left((h_\\theta(x_{i}) - y_{i}) x_{i}\\right) \\newline \\rbrace& \\end{align*}\n",
    "where m is the size of the training set, θ \n",
    "0\n",
    "​\t  a constant that will be changing simultaneously with θ1\n",
    "​\t  and xi, yi are values of the given training set (data).\n",
    "\n",
    "The point of all this is that if we start with a guess for our hypothesis and then repeatedly apply these gradient descent equations, our hypothesis will become more and more accurate.\n",
    "\n",
    "So, this is simply gradient descent on the original cost function J. This method looks at every example in the entire training set on every step, and is called batch gradient descent. \n",
    "\n",
    "Note that, while gradient descent can be susceptible to local minima in general, the optimization problem we have posed here for linear regression has only one global, and no other local, optima; thus gradient descent always converges (assuming the learning rate α is not too large) to the global minimum. Indeed, J is a convex quadratic function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Features\n",
    "\n",
    "Linear regression with multiple variables is also known as \"multivariate linear regression\".\n",
    "\n",
    "We now introduce notation for equations where we can have any number of input variables.\n",
    "\n",
    "\\begin{align*}x_j^{(i)} &= \\text{value of feature } j \\text{ in the }i^{th}\\text{ training example} \\newline x^{(i)}& = \\text{the input (features) of the }i^{th}\\text{ training example} \\newline m &= \\text{the number of training examples} \\newline n &= \\text{the number of features} \\end{align*}\n",
    "\n",
    "The multivariable form of the hypothesis function accommodating these multiple features is as follows:\n",
    "\n",
    "$hθ(x) = θ_0 + θ1 x_1 + θ2 x_2 + θ3 x_3 + ... + θn x_n$\n",
    "\n",
    "Using the definition of matrix multiplication, our multivariable hypothesis function can be concisely represented as:\n",
    "\n",
    "\\begin{align*}h_\\theta(x) =\\begin{bmatrix}\\theta_0 \\hspace{2em} \\theta_1 \\hspace{2em} ... \\hspace{2em} \\theta_n\\end{bmatrix}\\begin{bmatrix}x_0 \\newline x_1 \\newline \\vdots \\newline x_n\\end{bmatrix}= \\theta^T x\\end{align*}\n",
    "This is a vectorization of our hypothesis function for one training example; see the lessons on vectorization to learn more.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent for Multiple Variables\n",
    "\n",
    "The gradient descent equation itself is generally the same form; we just have to repeat it for our 'n' features:\n",
    "\n",
    "\\begin{align*} & \\text{repeat until convergence:} \\; \\lbrace \\newline \\; & \\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_0^{(i)}\\newline \\; & \\theta_1 := \\theta_1 - \\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_1^{(i)} \\newline \\; & \\theta_2 := \\theta_2 - \\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_2^{(i)} \\newline & \\cdots \\newline \\rbrace \\end{align*}\n",
    "In other words:\n",
    "\n",
    "\\begin{align*}& \\text{repeat until convergence:} \\; \\lbrace \\newline \\; & \\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_j^{(i)} \\; & \\text{for j := 0...n}\\newline \\rbrace\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent in Practice \n",
    "\n",
    "\n",
    "We can speed up gradient descent by having each of our input values in roughly the same range. This is because θ will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven.\n",
    "\n",
    "The way to prevent this is to modify the ranges of our input variables so that they are all roughly the same.\n",
    "\n",
    "Two techniques to help with this are feature scaling and mean normalization. Feature scaling involves dividing the input values by the range (i.e. the maximum value minus the minimum value) of the input variable, resulting in a new range of just 1. Mean normalization involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero.\n",
    "\n",
    "Note that dividing by the range or the standard deviation, give different results.\n",
    "\n",
    "### About learning rate and GD : \n",
    "\n",
    "If α is too small: slow convergence.\n",
    "If α is too large: may not decrease on every iteration and thus may not converge.\n",
    "Try and pick different value for alpha : 0.001, 0.005, 0.01, 0.05, 0.1 ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent in Practice II - Learning Rate\n",
    "\n",
    "Make a plot with number of iterations on the x-axis. Now plot the cost function, J(θ) over the number of iterations of gradient descent. If J(θ) ever increases, then you probably need to decrease α.\n",
    "\n",
    "Automatic convergence test : Declare convergence if J(θ) decreases by less than E in one iteration, where E is some small value such as 10^{−3}. However in practice it's difficult to choose this threshold value.\n",
    "\n",
    "<br>\n",
    "<img src='files/grad_descent.png'>\n",
    "\n",
    "It has been proven that if learning rate α is sufficiently small, then J(θ) will decrease on every iteration.\n",
    "\n",
    "\n",
    "To summarize:\n",
    "\n",
    "If $\\alpha$ is too small: slow convergence.\n",
    "\n",
    "If $\\alpha$ is too large: may not decrease on every iteration and thus may not converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features and Polynomial Regression\n",
    "\n",
    "We can improve our features and the form of our hypothesis function in a couple different ways.\n",
    "\n",
    "We can combine multiple features into one. For example, we can combine x1 and x2 into a new feature x3.\n",
    "\n",
    "### Polynomial Regression\n",
    "\n",
    "Our hypothesis function does not need to be linear if it doesn't fit the data well.\n",
    "\n",
    "We can change the behavior or curve of our hypothesis function by making it a quadratic, cubic or square root function (or any other form).\n",
    "\n",
    "One important thing to keep in mind is, if you choose your features this way then feature scaling becomes very important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Equation\n",
    "\n",
    "Gradient descent gives one way of minimizing J. Let’s discuss a second way of doing so, this time performing the minimization explicitly and without resorting to an iterative algorithm. \n",
    "\n",
    "In the \"Normal Equation\" method, we will minimize J by explicitly taking its derivatives with respect to the θj ’s, and setting them to zero. This allows us to find the optimum theta without iteration. \n",
    "\n",
    "The normal equation formula is given below:\n",
    "\n",
    "\n",
    "\\begin{align*}& \\theta = (X^T X)^{-1}X^T y\\end{align*}\n",
    "\n",
    "Caveats : \n",
    "- slow if n is large (n > 10000)\n",
    "- need to compute (X^T X)^{-1} which is very slow\n",
    "- if (X^T X) is not-invertible, we have problems (though we could use inverse approx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
