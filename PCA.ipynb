{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "### Motivation I: Data Compression\n",
    "\n",
    "We may want to reduce the dimension of our features if we have a lot of redundant data.\n",
    "\n",
    "To do this, we find two highly correlated features, plot them, and make a new line that seems to describe both features accurately.\n",
    "\n",
    "Doing dimensionality reduction will reduce the total data we have to store in computer memory and will speed up our learning algorithm.\n",
    "\n",
    "Note: in dimensionality reduction, we are reducing our features rather than our number of examples. Our variable m will stay the same size; n, the number of features each example carries will be reduced.\n",
    "\n",
    "### Motivation II: Visualization\n",
    "\n",
    "It is not easy to visualize data that is more than three dimensions. We can reduce the dimensions of our data to 3 or less in order to plot it.\n",
    "\n",
    "We need to find new features that can effectively summarize all the other features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Formulation\n",
    "\n",
    "The most popular dimensionality reduction algorithm is Principal Component Analysis (PCA)\n",
    "\n",
    "Given two features, we want to find a single line that effectively describes both features at once. We then map our old features onto this new line to get a new single feature. The same can be done with three features, where we map them to a plane.\n",
    "\n",
    "Reduce from n-dimension to k-dimension: \n",
    "- Find k vectors $u^{1} , u^{2}, … , u^{k}$ to determine the position of the hyperplane onto which to project the data so as to minimize the projection error.\n",
    "\n",
    "If we are converting from 3d to 2d, we will project our data onto two directions (a plane), so k will be 2.\n",
    "\n",
    "PCA is not linear regression :\n",
    "\n",
    "In linear regression, we are minimizing the squared error from every point to our predictor line. These are vertical distances.\n",
    "In PCA, we are minimizing the shortest orthogonal distances, to our data points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis Algorithm\n",
    "\n",
    "Before we can apply PCA, we have to standardize our dataset by subtracting every observation for every feature by the feature mean and divide it by the feature standard deviation.\n",
    "\n",
    "After this, PCA has two tasks: figure out the k eigenvectors $u^{1} , u^{2}, … , u^{k}$ and also to find $z_{1}, z_{2}, …,z_{m}$ for each m examples.\n",
    "\n",
    "### Compute the covariance matrix\n",
    "\n",
    "<img src='files/cov_mat.png'>\n",
    "\n",
    "### Compute the eigenvectors of the covariance matrix\n",
    "\n",
    "To do so we use the singular value decomposition technique applied to the covariance matrix.\n",
    "\n",
    "<img src='files/eigen.png'>\n",
    "\n",
    "Just take the first k-vectors from U (first k columns) and get the 'Ureduce' matrix. This will be an m×k matrix. \n",
    "\n",
    "We use it to compute our new features : $z = (Ureduce)^T * x$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction from Compressed Representation\n",
    "\n",
    "If we use PCA to compress our data, how can we uncompress our data, or go back to our original number of features?\n",
    "\n",
    "To go back from z in R to x in R², we can use the following equation :\n",
    "\n",
    "$x_{\\rm approx} =  Ureduce * z$\n",
    "\n",
    "Note that we can only get approximations of our original data.\n",
    "\n",
    "Note: It turns out that the U matrix has the special property that it is a Unitary Matrix. One of the special properties of a Unitary Matrix is :\n",
    "$U^{-1} = U^{T}$ (since we are dealing with real numbers here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the Number of Principal Components\n",
    "\n",
    "How do we choose k, also called the number of principal components?\n",
    "\n",
    "One way to choose k is by using the following formula :\n",
    "\n",
    "- Given the average squared projection error: $\\frac{1}{m}\\sum_{i=1}^m\\|x^{(i)}-x^{(i)}_\\text{approx}\\|^2$\n",
    "\n",
    "- Also given the total variation in the data : $\\frac{1}{m}\\sum_{i=1}^m\\|x^{(i)}\\|^2$\n",
    "\n",
    "- Choose k to be the smallest value such so that : $\\frac{ \\frac{1}{m} \\sum_{i=1}^m ||x^{(i)}- x^{(i)}_{\\rm approx}||^2}{\\frac{1}{m} \\sum_{i=1}^m ||x^{(i)}||^2} \\leq 0.01$ or $(0.05)$\n",
    "\n",
    "\n",
    "In other words, the squared projection error divided by the total variation should be less than one percent, so that 99% of the variance is\n",
    "retained.\n",
    "\n",
    "Algorithm for choosing k : \n",
    "1. Try PCA with k=1,2,…\n",
    "2. Compute $Ureduce, z, x$\n",
    "3. Check the formula given above that 99% of the variance is retained. If not, go to step one and increase k.\n",
    "This procedure would actually be horribly inefficient.\n",
    "\n",
    "But recall that when we used the covariance matrix to compute our eigenvectors we also got other matrices : \n",
    "\n",
    "U,S,V = svd(Sigma) where U is the matrix of the eigenvectors and S is the matrix of the eigenvalues.\n",
    "\n",
    "With that matrix S, we can actually check for 99% of retained variance as follows :\n",
    "\n",
    "<img src='files/remaining_var_PCA.png'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advice for Applying PCA\n",
    "\n",
    "The most common use of PCA is to speed up supervised learning.\n",
    "\n",
    "Given a training set with a large number of features, we can use PCA to reduce the number of features in each\n",
    "example of the training set.\n",
    "\n",
    "Note that we should define the PCA reduction only on the training set and not on the cross-validation or test sets. Then after this, you can apply the mapping to your cross-validation and test sets.\n",
    "\n",
    "Trying to prevent overfitting is a very bad use of PCA. \n",
    "\n",
    "It might work, but is not recommended because it does not consider the values of our label y. Using just regularization will be at least as effective and we will not lose as much intel.\n",
    "\n",
    "Don't assume you need to do PCA. Try your full machine learning algorithm without PCA first. Then use PCA if you find that you need it.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
