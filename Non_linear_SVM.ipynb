{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non linear SVM\n",
    "\n",
    "\n",
    "## Kernels\n",
    "\n",
    "Kernels allow us to make complex, non-linear classifiers using Support Vector Machines.\n",
    "\n",
    "Given x, we compute new features depending on proximity to landmarks.\n",
    "\n",
    "To do this, we find the \"similarity\" of x and some landmark : $l^{(1)}$, $l^{(2)}$, $l^{(3)}$\n",
    "\n",
    "To do this, we nd the \"similarity\" of x and some landmark $l^{(i)}$ :\n",
    "\n",
    "<img src ='files/kernel_rbf.png'>\n",
    "\n",
    "This \"similarity\" function is called a Gaussian Kernel. It is a specific example of a kernel.\n",
    "\n",
    "The similarity function can also be written as follows :\n",
    "\n",
    "<img src ='files/kernel_rbf2.png'>\n",
    "\n",
    "There are a couple properties of the similarity function :\n",
    "\n",
    "<img src ='files/kernel_rbf3.png'>\n",
    "\n",
    "In other words, if x and the landmark are close, then the similarity will be close to 1, and if x and the landmark are far away from each other, the similarity will be close to 0.\n",
    "\n",
    "Each landmark gives us the features in our hypothesis :\n",
    "\n",
    "<img src ='files/rbf_features_hp.png'>\n",
    "\n",
    "The kernel trick, maps data (sometimes nonlinear data) from a low-dimensional space to a high-dimensional space. In a higher dimension, you can solve a linear problem that’s nonlinear in lower-dimensional space.\n",
    "\n",
    "$σ^{2}$ is a parameter of the Gaussian Kernel, and it can be modified to increase or decrease the drop-off of our feature. \n",
    "\n",
    "Combined with looking at the values inside Θ, we can choose these landmarks to get the general shape of the decision boundary.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernels II\n",
    "\n",
    "One way to get the landmarks is to put them in the exact same locations as all the training examples. This gives us m landmarks, with one landmark per training example.\n",
    "\n",
    "Given example x:\n",
    "\n",
    "<img src ='files/f_features.png'>\n",
    "\n",
    "This gives us a \"feature vector\" $f_{i}$ of all our features for example $x_{i}$ . We may also set $f_{0} =1$ to correspond with $θ_0$. \n",
    "Thus given training example $x_{i}$ :\n",
    "\n",
    "<img src ='files/f_features_xi.png'>\n",
    "\n",
    "Now to get the parameters Θ we can use the SVM minimization algorithm but with $f_{i}$ substituted in for $x_{i}$ :\n",
    "\n",
    "<br>\n",
    "<img src ='files/cost_fi.png'>\n",
    "<br>\n",
    "\n",
    "Using kernels to generate $f_{i}$ is not exclusive to SVMs and may also be applied to logistic regression. \n",
    "\n",
    "However, because of computational optimizations specific to SVMs, kernels combined with SVMs are much faster than with other algorithms.\n",
    "\n",
    "<img src ='files/svm_params.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using An SVM\n",
    "\n",
    "In practical application, the choices you do need to make are:\n",
    "- Choice of parameter C\n",
    "- Choice of kernel (similarity function) :\n",
    "    - No kernel (\"linear\" kernel) gives standard linear classifier\n",
    "        --> Choose when n is large and when m is small\n",
    "    - Gaussian Kernel (above) where you need to choose σ²\n",
    "        --> Choose when n is small and m is large (but not above 50K)\n",
    "\n",
    "Note: do perform feature scaling before using the Gaussian Kernel, in order to improve convergence and give each feature the same importance \n",
    "\n",
    "Note: not all similarity functions are valid kernels. They must satisfy \"Mercer's Theorem\" which guarantees that the SVM package's optimizations run correctly and do not diverge.\n",
    "\n",
    "### Multi-class Classification\n",
    "\n",
    "You can use the one-vs-all method just like we did for logistic regression where we train k SVM, one for each class vs the others :\n",
    "\n",
    "<img src ='files/multi_svm.png'>\n",
    "\n",
    "### Logistic Regression vs. SVMs\n",
    "\n",
    "If n is large (relatively to m), then use logistic regression, or SVM without a kernel (the \"linear kernel\") : a simpler model will not overfit the data given the low count of training examples\n",
    "\n",
    "If n is small (less than 1k) and m is intermediate (between 10 and 50K), then use SVM with a Gaussian Kernel : you can afford a more complicated hypothesis.\n",
    "\n",
    "If n is small and m is large (more than 100K), then you need manually create/add more features, then use logistic regression or SVM without a kernel.\n",
    "\n",
    "In the first case, we don't have enough examples to need a complicated polynomial hypothesis. In the second example, we have enough examples so we may need a complex non-linear hypothesis. In the last case, we want to increase our features so that logistic regression becomes applicable.\n",
    "\n",
    "End note : a neural network is likely to work well for any of these situations, but may be slower to train.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
