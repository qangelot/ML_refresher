{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support vector machine\n",
    "\n",
    "## Optimization Objective\n",
    "\n",
    "\n",
    "To make a support vector machine, we will refer to the cost function of logistic regression but modify the first term so that when $θ^Tx$ is greater than 1, it outputs 0. \n",
    "\n",
    "Furthermore, for values less than 1, we shall use a straight decreasing line instead of the sigmoid curve (Hinge loss function).\n",
    "\n",
    "Similarly, we modify the second term of the cost function so that when $θ^Tx$ is less than -1, it outputs 0.\n",
    "\n",
    "We also modify it so that for values greater than -1, we use a straight increasing line instead of the sigmoid curve.\n",
    "\n",
    "<img src='files/svm_cost.png'>\n",
    "\n",
    "The cost function of linear SVM is as follow : \n",
    "\n",
    "<img src='files/cost_functionSVM.png'>\n",
    "\n",
    "The convention dictates that we regularize using a factor C, instead of λ, like so and C = 1 / λ.\n",
    "\n",
    "Now, when we wish to regularize more (that is, reduce overfiting), we decrease C, and when we wish to regularize less (that is, reduce underfitting), we increase C.\n",
    "\n",
    "Finally, note that the hypothesis of the Support Vector Machine is not interpreted as the probability of y being 1 or 0 (as it is for the hypothesis of logistic regression). Instead, it outputs either 1 or 0 :\n",
    "\n",
    "<img src='files/svm_hp.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The large margin classifier\n",
    "\n",
    "<br>\n",
    "<img src='files/cost_functionSVM.png'>\n",
    "\n",
    "Let's say we have a very high value of C, then we need to make the non regularized part of the equation as small as possible in order to minimize the SVM cost function.\n",
    "\n",
    "So, in that case, the goal is that whenever we have a training example with a label of y = 1, we want that first term to be zero, what we need is to find a value of theta so that $θ^Tx$ is greater than or equal to 1. \n",
    "\n",
    "And similarly, whenever we have an example, with label zero, in order to make sure that the second term is 0, we need that $θ^Tx$ is less than or equal to -1.\n",
    "\n",
    "Knowing that, the goal of the SVM is to maximize the distance (also called the margin of the support vector machine) between the two classes and this gives the SVM a certain robustness, because it tries to separate the data with as large a margin as possible.\n",
    "\n",
    "In practice when applying support vector machines, when C is not very large like that, it can do a better job ignoring the few outliers. And also do reasonable things even if your data is not linearly separable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maths behind SVM\n",
    "\n",
    "The length of vector v is denoted $\\left\\|v\\right\\|$, and it describes the line on a graph from origin $(0,0)$ to ($v_{1}$, $v_{2}$).\n",
    "The length of vector v can be calculated with the Pythagorean theorem.\n",
    "\n",
    "The projection of vector v onto vector u is found by taking a right angle from the end of v to u, creating a right triangle :\n",
    "\n",
    "p= length of projection of v onto the vector u\n",
    "\n",
    "$u^Tv$ = $v^Tu$ = p⋅$\\left\\|u\\right\\|$ = $u_{1}v_{1}$ + $u_{2}v_{2}$\n",
    "\n",
    "If the angle between the lines for v and u is greater than 90 degrees, then the projection p will be negative.\n",
    "\n",
    "\n",
    "\\begin{align*} \\min_\\theta & \\frac{1}{2}\\sum_{j=1}^n\\theta_j^2\\\\ \\mbox{s.t.} & \\|\\theta\\|\\cdot p^{(i)} \\geq 1\\quad \\mbox{if}\\ y^{(i)} = 1\\\\ & \\|\\theta\\|\\cdot p^{(i)} \\leq -1\\quad \\mbox{if}\\ y^{(i)} = 0 \\end{align*}\n",
    "\n",
    "where $p^{(i)}$\n",
    "  is the (signed - positive or negative) projection of $x^{(i)}$\n",
    "  onto $\\theta$.\n",
    "  \n",
    "\\begin{align*} \\min_\\theta & \\frac{1}{2}\\sum_{j=1}^n\\theta_j^2\\\\ = \n",
    "1/2 * (Θ²_{1} + Θ²_{2} + ... + Θ²_{n}) \n",
    "= 1/2 * (\\sqrt{Θ²_{1} + Θ²_{2} + ... + Θ²_{n}} )²  = 1/2 * \\|Θ\\|²  \\end{align*}\n",
    "\n",
    "\n",
    "We can use the same rules to rewrite :\n",
    "\n",
    "\\begin{align*} θ^T = p^{(i)} . \\|Θ\\| = Θ_{1}x^{(i)}_{1} + Θ_{2}x^{(i)}_{2}  + ... + Θ_{n}x^{(i)}_{n} \\end{align*}\n",
    "\n",
    "If y=1, we want :\n",
    "\\begin{align*} p^{(i)} . \\|Θ\\| >= 1 \\end{align*}\n",
    "If y=0, we want :\n",
    "\\begin{align*} p^{(i)} . \\|Θ\\| <= -1  \\end{align*}\n",
    "\n",
    "\n",
    "<br>\n",
    "<img src='files/svm_db1.png'>\n",
    "<br>\n",
    "\n",
    "The reason this causes a \"large margin\" is because it is proven by linear algebra that the vector for Θ is perpendicular to the decision boundary. And in order for our optimization\n",
    "objective to hold true, we need the absolute value of our $p^{(i)}$, projections of $x^{(i)}$, to be as large as possible. In the image above, the $p^{(i)}$ are small and then the margin of the decision boundary in regard to the $x^{(i)}$ is very small too. \n",
    "It's the contrary on the picture below :\n",
    "\n",
    "<br>\n",
    "<img src='files/svm_db2.png'>\n",
    "<br>\n",
    "\n",
    "If $Θ_{0} = 0$, then all our decision boundaries will intersect $(0,0)$. If $Θ_{0} \t\\neq 0$, the support vector machine will still find a large margin for the decision\n",
    "boundary.\n",
    "\n",
    "All this demonstration is based on a large value of C, the regularization parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
